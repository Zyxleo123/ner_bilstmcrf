{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: http_proxy=127.0.0.1:7890\n",
      "env: https_proxy=127.0.0.1:7890\n"
     ]
    }
   ],
   "source": [
    "%env http_proxy 127.0.0.1:7890\n",
    "%env https_proxy 127.0.0.1:7890"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/mambaforge/envs/bilstmcrf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, builder\n",
    "from pytorch_lightning import Trainer\n",
    "from argparse import ArgumentParser\n",
    "from src.dataset import NERDataset\n",
    "from src.collator import NERDataCollator\n",
    "from src.pl_module import LightningBiLSTMCRF\n",
    "from src.variable import LABEL_TO_IDX, PAD_LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tokenizer...\n",
      "Initializing model...\n",
      "Initializing dataset...\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2/2 [00:00<00:00, 136.60 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 319.15 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "print(\"Initializing model...\")\n",
    "\n",
    "char_level = False\n",
    "model = LightningBiLSTMCRF(LABEL_TO_IDX, 1, 128, \n",
    "                        bert_lr=0, lstm_lr=0, crf_lr=0,\n",
    "                        char_level=char_level)\n",
    "print(\"Initializing dataset...\")\n",
    "train_dataset_name = \"toy\"\n",
    "train_dataset = NERDataset(train_dataset_name, LABEL_TO_IDX)\n",
    "val_dataset_name = \"toy\"\n",
    "val_dataset = NERDataset(val_dataset_name, LABEL_TO_IDX)\n",
    "def train_generator():\n",
    "    for i in range(len(train_dataset)):\n",
    "        yield {\"text\": train_dataset.text[i], \"labels\": train_dataset.labels[i]}\n",
    "def val_generator():\n",
    "    for i in range(len(val_dataset)):\n",
    "        yield {\"text\": val_dataset.text[i], \"labels\": val_dataset.labels[i]}\n",
    "train_dataset = Dataset.from_generator(train_generator)\n",
    "val_dataset = Dataset.from_generator(val_generator)\n",
    "print(\"Tokenizing dataset...\")\n",
    "def tokenize(example):\n",
    "    encoding = tokenizer(example[\"text\"], is_split_into_words=True)\n",
    "    encoding['word_ids'] = [encoding.word_ids(b) for b in range(len(example['labels']))]\n",
    "    if  not char_level:\n",
    "        encoding['labels'] = example['labels']\n",
    "    else:\n",
    "        # align labels with word_ids\n",
    "        labels = example['labels']\n",
    "        word_ids = encoding['word_ids']\n",
    "        new_labels = []\n",
    "        for b in range(len(labels)):\n",
    "            new_label = []\n",
    "            for w in word_ids[b]:\n",
    "                if w is not None:\n",
    "                    new_label.append(labels[b][w])\n",
    "                else:\n",
    "                    new_label.append('O')\n",
    "            new_labels.append(new_label)\n",
    "        encoding['labels'] = new_labels\n",
    "    return encoding\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=4, remove_columns=[\"text\"])\n",
    "val_dataset = val_dataset.map(tokenize, batched=True, batch_size=4, remove_columns=[\"text\"])\n",
    "print(\"Training model...\")\n",
    "collator = NERDataCollator(tokenizer=tokenizer)\n",
    "train_loader = DataLoader(train_dataset, collate_fn=collator, batch_size=4)\n",
    "val_loader = DataLoader(val_dataset, collate_fn=collator, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids(Decoded):\n",
      "[CLS] 上 海 市 场 。 [SEP]\n",
      "[CLS] 腾 讯 游 戏 [SEP] [PAD]\n",
      "--------------------------------------------------\n",
      "Attention mask:\n",
      "tensor([1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 0])\n",
      "--------------------------------------------------\n",
      "Labels:\n",
      "B-GPE O <PAD> \n",
      "B-ORG M-ORG E-GPE \n",
      "--------------------------------------------------\n",
      "Word ids:\n",
      "tensor([-1,  0,  0,  0,  0,  1, -1])\n",
      "tensor([-1,  0,  1,  2,  2, -1, -1])\n"
     ]
    }
   ],
   "source": [
    "from src.variable import IDX_TO_LABEL\n",
    "input = next(iter(train_loader))\n",
    "print(\"Input ids(Decoded):\")\n",
    "for input_id in input[\"input_ids\"]:\n",
    "    print(tokenizer.decode(input_id))\n",
    "print(\"-\"*50)\n",
    "print(\"Attention mask:\")\n",
    "for attention_mask in input[\"attention_mask\"]:\n",
    "    print(attention_mask)\n",
    "print(\"-\"*50)\n",
    "print(\"Labels:\")\n",
    "for labels in input[\"labels\"]:\n",
    "    for label in labels:\n",
    "        print(IDX_TO_LABEL[label.item()], end=' ')\n",
    "    print()\n",
    "print(\"-\"*50)\n",
    "print(\"Word ids:\")\n",
    "for word_ids in input[\"word_ids\"]:\n",
    "    print(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding time: 0.03s\n",
      "Convert time: 0.00s\n",
      "CRF forward time: 0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.0255, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.calculate_loss(**input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding time: 0.03s\n",
      "Convert time: 0.00s\n",
      "CRF decode time: 0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[15, 3], [11, 3, 3]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.predict(input['input_ids'], input['attention_mask'], input['word_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "[CLS] 上 海 市 场 。 [SEP]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m to_entity \u001b[38;5;241m=\u001b[39m labels[t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (from_entity\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS-\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (to_entity\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM-\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m to_entity\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE-\u001b[39m\u001b[38;5;124m'\u001b[39m))), tokenizer\u001b[38;5;241m.\u001b[39mdecode(encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (from_entity\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB-\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m to_entity\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB-\u001b[39m\u001b[38;5;124m'\u001b[39m)), tokenizer\u001b[38;5;241m.\u001b[39mdecode(encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_entity\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB-\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (to_entity\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM-\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m to_entity\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE-\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m from_entity[\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;241m==\u001b[39m to_entity[\u001b[38;5;241m2\u001b[39m:], tokenizer\u001b[38;5;241m.\u001b[39mdecode(encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mAssertionError\u001b[0m: [CLS] 上 海 市 场 。 [SEP]"
     ]
    }
   ],
   "source": [
    "# confirm that no from S-xxx to M-*/E-*\n",
    "# no from B-xxx to B-*/M-yyy/E-yyy/O/<STOP>\n",
    "# no from M-xxx to B-*/M-yyy/E-yyy/O/<STOP>\n",
    "# no from E-* to M-*/E-*/\n",
    "# no from O to M-*/E-*\n",
    "# no from START_LABEL to M-*/E-*\n",
    "# no from B-*/M-* to STOP_LABEL\n",
    "for encoding in train_dataset:\n",
    "    labels = encoding['labels']\n",
    "    for t in range(len(labels) - 1):\n",
    "        from_entity = labels[t]\n",
    "        to_entity = labels[t + 1]\n",
    "        assert not (from_entity.startswith('S-') and (to_entity.startswith('M-') or to_entity.startswith('E-'))), tokenizer.decode(encoding['input_ids'])\n",
    "        assert not (from_entity.startswith('B-') and to_entity.startswith('B-')), tokenizer.decode(encoding['input_ids'])\n",
    "        if from_entity.startswith('B-') and (to_entity.startswith('M-') or to_entity.startswith('E-')):\n",
    "            assert from_entity[2:] == to_entity[2:], tokenizer.decode(encoding['input_ids'])\n",
    "        assert not (from_entity.startswith('M-') and to_entity.startswith('B-')), tokenizer.decode(encoding['input_ids'])\n",
    "        if from_entity.startswith('M-') and (to_entity.startswith('M-') or to_entity.startswith('E-')):\n",
    "            assert from_entity[2:] == to_entity[2:], tokenizer.decode(encoding['input_ids'])\n",
    "        assert not (from_entity.startswith('E-') and (to_entity.startswith('M-') or to_entity.startswith('E-'))), tokenizer.decode(encoding['input_ids'])\n",
    "        assert not (from_entity == 'O' and (to_entity.startswith('M-') or to_entity.startswith('E-'))), tokenizer.decode(encoding['input_ids'])\n",
    "        if t == 0: \n",
    "            assert not (from_entity.startswith('M-') or from_entity.startswith('E-'))\n",
    "        if t == len(labels) - 2:\n",
    "            assert not (to_entity.startswith('E-') or to_entity.startswith('M-'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "lstm = nn.LSTM(2, 10, 2, bidirectional=True, batch_first=True, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0090,  0.0097,  0.0124,  0.0273,  0.0010, -0.0052,  0.0005,  0.0159,\n",
       "          -0.0103,  0.0107, -0.0036, -0.0376,  0.0099,  0.0062,  0.0067, -0.0009,\n",
       "          -0.0074,  0.0110, -0.0308,  0.0083],\n",
       "         [ 0.0037,  0.0124,  0.0101,  0.0176,  0.0008, -0.0032, -0.0060,  0.0124,\n",
       "          -0.0057,  0.0100,  0.0050, -0.0124, -0.0004,  0.0034,  0.0033, -0.0007,\n",
       "          -0.0051,  0.0049, -0.0048,  0.0021]], grad_fn=<SqueezeBackward1>),\n",
       " (tensor([[ 0.0752, -0.0223, -0.0282, -0.0005,  0.0200,  0.0265,  0.0407,  0.0272,\n",
       "           -0.0415,  0.0149],\n",
       "          [ 0.0646,  0.0357,  0.0567,  0.0488, -0.0922,  0.0379,  0.1003,  0.0322,\n",
       "           -0.0094, -0.0836],\n",
       "          [ 0.0037,  0.0124,  0.0101,  0.0176,  0.0008, -0.0032, -0.0060,  0.0124,\n",
       "           -0.0057,  0.0100],\n",
       "          [-0.0036, -0.0376,  0.0099,  0.0062,  0.0067, -0.0009, -0.0074,  0.0110,\n",
       "           -0.0308,  0.0083]], grad_fn=<SqueezeBackward1>),\n",
       "  tensor([[ 0.1501, -0.0443, -0.0553, -0.0010,  0.0393,  0.0527,  0.0842,  0.0539,\n",
       "           -0.0857,  0.0300],\n",
       "          [ 0.1495,  0.0693,  0.1215,  0.0819, -0.1995,  0.0631,  0.2254,  0.0848,\n",
       "           -0.0176, -0.1732],\n",
       "          [ 0.0074,  0.0250,  0.0203,  0.0347,  0.0015, -0.0064, -0.0122,  0.0248,\n",
       "           -0.0115,  0.0199],\n",
       "          [-0.0072, -0.0708,  0.0194,  0.0120,  0.0136, -0.0017, -0.0146,  0.0206,\n",
       "           -0.0598,  0.0168]], grad_fn=<SqueezeBackward1>)))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([[1, 1],\n",
    "                    [0, 0]], dtype=torch.float32)\n",
    "lstm(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4, 5],\n",
       "         [4, 5]],\n",
       "\n",
       "        [[2, 3],\n",
       "         [0, 1]],\n",
       "\n",
       "        [[2, 3],\n",
       "         [2, 3]],\n",
       "\n",
       "        [[0, 1],\n",
       "         [0, 1]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "L, B = 4, 2\n",
    "a = torch.arange(8).view(4, 2)\n",
    "mask = torch.tensor([[2, 1],\n",
    "                    [1, 0],\n",
    "                    [1, 1],\n",
    "                    [0, 0]])\n",
    "a[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5],\n",
       "        [6, 7]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
