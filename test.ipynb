{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env http_proxy 127.0.0.1:7890\n",
    "%env https_proxy 127.0.0.1:7890"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, builder\n",
    "from pytorch_lightning import Trainer\n",
    "from argparse import ArgumentParser\n",
    "from src.dataset import NERDataset\n",
    "from src.collator import NERDataCollator\n",
    "from src.pl_module import LightningBiLSTMCRF\n",
    "from src.variable import LABEL_TO_IDX, PAD_LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "print(\"Initializing model...\")\n",
    "\n",
    "char_level = False\n",
    "model = LightningBiLSTMCRF(LABEL_TO_IDX, 1, 128, \n",
    "                        bert_lr=0, lstm_lr=0, crf_lr=0,\n",
    "                        char_level=char_level)\n",
    "print(\"Initializing dataset...\")\n",
    "train_dataset_name = \"toy\"\n",
    "train_dataset = NERDataset(train_dataset_name, LABEL_TO_IDX)\n",
    "val_dataset_name = \"toy\"\n",
    "val_dataset = NERDataset(val_dataset_name, LABEL_TO_IDX)\n",
    "def train_generator():\n",
    "    for i in range(len(train_dataset)):\n",
    "        yield {\"text\": train_dataset.text[i], \"labels\": train_dataset.labels[i]}\n",
    "def val_generator():\n",
    "    for i in range(len(val_dataset)):\n",
    "        yield {\"text\": val_dataset.text[i], \"labels\": val_dataset.labels[i]}\n",
    "train_dataset = Dataset.from_generator(train_generator)\n",
    "val_dataset = Dataset.from_generator(val_generator)\n",
    "print(\"Tokenizing dataset...\")\n",
    "def tokenize(example):\n",
    "    encoding = tokenizer(example[\"text\"], is_split_into_words=True)\n",
    "    encoding['word_ids'] = [encoding.word_ids(b) for b in range(len(example['labels']))]\n",
    "    if  not char_level:\n",
    "        encoding['labels'] = example['labels']\n",
    "    else:\n",
    "        # align labels with word_ids\n",
    "        labels = example['labels']\n",
    "        word_ids = encoding['word_ids']\n",
    "        new_labels = []\n",
    "        for b in range(len(labels)):\n",
    "            new_label = []\n",
    "            for w in word_ids[b]:\n",
    "                if w is not None:\n",
    "                    new_label.append(labels[b][w])\n",
    "                else:\n",
    "                    new_label.append('O')\n",
    "            new_labels.append(new_label)\n",
    "        encoding['labels'] = new_labels\n",
    "    return encoding\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=4, remove_columns=[\"text\"])\n",
    "val_dataset = val_dataset.map(tokenize, batched=True, batch_size=4, remove_columns=[\"text\"])\n",
    "print(\"Training model...\")\n",
    "collator = NERDataCollator(tokenizer=tokenizer)\n",
    "train_loader = DataLoader(train_dataset, collate_fn=collator, batch_size=4)\n",
    "val_loader = DataLoader(val_dataset, collate_fn=collator, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.variable import IDX_TO_LABEL\n",
    "input = next(iter(train_loader))\n",
    "print(\"Input ids(Decoded):\")\n",
    "for input_id in input[\"input_ids\"]:\n",
    "    print(tokenizer.decode(input_id))\n",
    "print(\"-\"*50)\n",
    "print(\"Attention mask:\")\n",
    "for attention_mask in input[\"attention_mask\"]:\n",
    "    print(attention_mask)\n",
    "print(\"-\"*50)\n",
    "print(\"Labels:\")\n",
    "for labels in input[\"labels\"]:\n",
    "    for label in labels:\n",
    "        print(IDX_TO_LABEL[label.item()], end=' ')\n",
    "    print()\n",
    "print(\"-\"*50)\n",
    "print(\"Word ids:\")\n",
    "for word_ids in input[\"word_ids\"]:\n",
    "    print(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.calculate_loss(**input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.predict(input['input_ids'], input['attention_mask'], input['word_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that no from S-xxx to M-*/E-*\n",
    "# no from B-xxx to B-*/M-yyy/E-yyy/O/<STOP>\n",
    "# no from M-xxx to B-*/M-yyy/E-yyy/O/<STOP>\n",
    "# no from E-* to M-*/E-*/\n",
    "# no from O to M-*/E-*\n",
    "# no from START_LABEL to M-*/E-*\n",
    "# no from B-*/M-* to STOP_LABEL\n",
    "for encoding in train_dataset:\n",
    "    labels = encoding['labels']\n",
    "    for t in range(len(labels) - 1):\n",
    "        from_entity = labels[t]\n",
    "        to_entity = labels[t + 1]\n",
    "        assert not (from_entity.startswith('S-') and (to_entity.startswith('M-') or to_entity.startswith('E-'))), tokenizer.decode(encoding['input_ids'])\n",
    "        assert not (from_entity.startswith('B-') and to_entity.startswith('B-')), tokenizer.decode(encoding['input_ids'])\n",
    "        if from_entity.startswith('B-') and (to_entity.startswith('M-') or to_entity.startswith('E-')):\n",
    "            assert from_entity[2:] == to_entity[2:], tokenizer.decode(encoding['input_ids'])\n",
    "        assert not (from_entity.startswith('M-') and to_entity.startswith('B-')), tokenizer.decode(encoding['input_ids'])\n",
    "        if from_entity.startswith('M-') and (to_entity.startswith('M-') or to_entity.startswith('E-')):\n",
    "            assert from_entity[2:] == to_entity[2:], tokenizer.decode(encoding['input_ids'])\n",
    "        assert not (from_entity.startswith('E-') and (to_entity.startswith('M-') or to_entity.startswith('E-'))), tokenizer.decode(encoding['input_ids'])\n",
    "        assert not (from_entity == 'O' and (to_entity.startswith('M-') or to_entity.startswith('E-'))), tokenizer.decode(encoding['input_ids'])\n",
    "        if t == 0: \n",
    "            assert not (from_entity.startswith('M-') or from_entity.startswith('E-'))\n",
    "        if t == len(labels) - 2:\n",
    "            assert not (to_entity.startswith('E-') or to_entity.startswith('M-'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "lstm = nn.LSTM(2, 10, 2, bidirectional=True, batch_first=True, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor([[1, 1],\n",
    "                    [0, 0]], dtype=torch.float32)\n",
    "lstm(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "L, B = 4, 2\n",
    "a = torch.arange(8).view(4, 2)\n",
    "mask = torch.tensor([[2, 1],\n",
    "                    [1, 0],\n",
    "                    [1, 1],\n",
    "                    [0, 0]])\n",
    "a[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env http_proxy 127.0.0.1:7890\n",
    "%env https_proxy 127.0.0.1:7890\n",
    "import os\n",
    "from src.dataset import NERDataset\n",
    "from src.collator import NERDataCollator\n",
    "from src.pl_module import LightningBiLSTMCRF\n",
    "from src.variable import LABEL_TO_IDX\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, builder\n",
    "from pytorch_lightning import Trainer\n",
    "builder.has_sufficient_disk_space = lambda needed_bytes, directory=\".\": True\n",
    "print(\"Initializing tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"bert-base-chinese\")\n",
    "print(\"Initializing dataset...\")\n",
    "test_dataset = NERDataset('test', LABEL_TO_IDX)\n",
    "def test_generator():\n",
    "    for i in range(len(test_dataset)):\n",
    "        yield {\"text\": test_dataset.text[i]}\n",
    "test_dataset = Dataset.from_generator(test_generator)\n",
    "print(\"Tokenizing dataset...\")\n",
    "def tokenize(example):\n",
    "    encoding = tokenizer(example[\"text\"], is_split_into_words=True)\n",
    "    encoding['word_ids'] = [encoding.word_ids(b) for b in range(len(example['text']))]\n",
    "    return encoding\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, remove_columns=[\"text\"], batch_size=32)\n",
    "print(\"Initializing dataloader...\")\n",
    "collator = NERDataCollator(tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, collate_fn=collator, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(test_dataloader))['input_ids'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import NERDataset\n",
    "from src.variable import LABEL_TO_IDX\n",
    "ds = NERDataset('train', LABEL_TO_IDX, upsample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: http_proxy=127.0.0.1:7890\n",
      "env: https_proxy=127.0.0.1:7890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/mambaforge/envs/bilstmcrf/lib/python3.8/site-packages/huggingface_hub/file_download.py:1003: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.00 MB. The target location /root/.cache/huggingface/hub only has 0.00 MB free disk space.\n",
      "  warnings.warn(\n",
      "/root/mambaforge/envs/bilstmcrf/lib/python3.8/site-packages/huggingface_hub/file_download.py:1003: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.00 MB. The target location /root/.cache/huggingface/hub/models--ckiplab--bert-base-chinese-ner/blobs only has 0.00 MB free disk space.\n",
      "  warnings.warn(\n",
      "/root/mambaforge/envs/bilstmcrf/lib/python3.8/site-packages/huggingface_hub/file_download.py:1003: UserWarning: Not enough free disk space to download the file. The expected file size is: 407.02 MB. The target location /root/.cache/huggingface/hub only has 0.00 MB free disk space.\n",
      "  warnings.warn(\n",
      "/root/mambaforge/envs/bilstmcrf/lib/python3.8/site-packages/huggingface_hub/file_download.py:1003: UserWarning: Not enough free disk space to download the file. The expected file size is: 407.02 MB. The target location /root/.cache/huggingface/hub/models--ckiplab--bert-base-chinese-ner/blobs only has 0.00 MB free disk space.\n",
      "  warnings.warn(\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ckiplab/bert-base-chinese-ner and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "%env http_proxy 127.0.0.1:7890\n",
    "%env https_proxy 127.0.0.1:7890\n",
    "from transformers import (\n",
    "  BertTokenizerFast,\n",
    "  AutoModel,\n",
    ")\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "model = AutoModel.from_pretrained('ckiplab/bert-base-chinese-ner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**tokenizer('我叫沃尔夫冈，我住在柏林。', return_tensors='pt')).last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "sh\n",
      "##ang\n",
      "##hai\n",
      "[SEP]\n"
     ]
    }
   ],
   "source": [
    "for input_id in tokenizer('shanghai').input_ids:\n",
    "    print(tokenizer.decode(input_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
