{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, builder\n",
    "from pytorch_lightning import Trainer\n",
    "from argparse import ArgumentParser\n",
    "from src.dataset import NERDataset\n",
    "from src.collator import NERDataCollator\n",
    "from src.pl_module import LightningBiLSTMCRF\n",
    "from src.variable import LABEL_TO_IDX, PAD_LABEL\n",
    "builder.has_sufficient_disk_space = lambda needed_bytes, directory=\".\": True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env http_proxy 127.0.0.1:7890\n",
    "%env https_proxy 127.0.0.1:7890\n",
    "print(\"Initializing tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('ckiplab/bert-base-chinese-ner')\n",
    "print(\"Initializing dataset...\")\n",
    "train_dataset_name = \"toy\"\n",
    "train_dataset = NERDataset(train_dataset_name, LABEL_TO_IDX, upsample=False)\n",
    "val_dataset_name = \"dev\"\n",
    "val_dataset = NERDataset(val_dataset_name, LABEL_TO_IDX, upsample=False)\n",
    "def train_generator():\n",
    "    for i in range(len(train_dataset)):\n",
    "        yield {\"text\": train_dataset.text[i], \"labels\": train_dataset.labels[i]}\n",
    "def val_generator():\n",
    "    for i in range(len(val_dataset)):\n",
    "        yield {\"text\": val_dataset.text[i], \"labels\": val_dataset.labels[i]}\n",
    "train_dataset = Dataset.from_generator(train_generator)\n",
    "val_dataset = Dataset.from_generator(val_generator)\n",
    "print(\"Tokenizing dataset...\")\n",
    "def tokenize(example):\n",
    "    encoding = tokenizer(example[\"text\"], is_split_into_words=True)\n",
    "    encoding['word_ids'] = [encoding.word_ids(b) for b in range(len(example['labels']))]\n",
    "    encoding['word_ids'] = [list(map(lambda x: -1 if x is None else x, word_id)) for word_id in encoding['word_ids']]\n",
    "    encoding['labels'] = [[LABEL_TO_IDX[y] for y in b] for b in example['labels']]\n",
    "    return encoding\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=32, remove_columns=[\"text\"])\n",
    "val_dataset = val_dataset.map(tokenize, batched=True, batch_size=32, remove_columns=[\"text\"])\n",
    "collator = NERDataCollator(tokenizer=tokenizer)\n",
    "train_loader = DataLoader(train_dataset, collate_fn=collator, batch_size=32, num_workers=47, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, collate_fn=collator, batch_size=32, num_workers=47)\n",
    "print(\"Initializing model...\")\n",
    "model = LightningBiLSTMCRF(LABEL_TO_IDX, 1, 256, \n",
    "                        bert_lr=0.0, lr=3e-5,\n",
    "                        optimizer='adamw', scheduler='onecycle',\n",
    "                        pretrained_model_name='ckiplab/bert-base-chinese-ner', freeze_bert=True,\n",
    "                        epochs=20, steps_per_epoch=len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids(Decoded):\n",
      "[CLS] 上 海 市 市 长 。 [SEP]\n",
      "[CLS] 腾 讯 游 戏 [SEP]\n",
      "Attention mask:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1]\n",
      "Word ids:\n",
      "[-1, 0, 0, 0, 1, 1, 2, -1]\n",
      "[-1, 0, 1, 2, 2, -1]\n",
      "labels:\n",
      "[1, 0, 0]\n",
      "[3, 6, 4]\n"
     ]
    }
   ],
   "source": [
    "print(\"Input ids(Decoded):\")\n",
    "for data in train_dataset:\n",
    "    print(tokenizer.decode(data['input_ids']))\n",
    "print(\"Attention mask:\")\n",
    "for data in train_dataset:\n",
    "    print(data['attention_mask'])\n",
    "print(\"Word ids:\")\n",
    "for data in train_dataset:\n",
    "    print(data['word_ids'])\n",
    "print(\"labels:\")\n",
    "for data in train_dataset:\n",
    "    print(data['labels'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids(Decoded):\n",
      "[CLS] 上 海 市 市 长 。 [SEP]\n",
      "[CLS] 腾 讯 游 戏 [SEP] [PAD] [PAD]\n",
      "--------------------------------------------------\n",
      "Attention mask:\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 0, 0])\n",
      "--------------------------------------------------\n",
      "Labels:\n",
      "S-GPE O O \n",
      "B-ORG M-ORG E-ORG \n",
      "--------------------------------------------------\n",
      "Word ids:\n",
      "tensor([-1,  0,  0,  0,  1,  1,  2, -1])\n",
      "tensor([-1,  0,  1,  2,  2, -1, -1, -1])\n"
     ]
    }
   ],
   "source": [
    "from src.variable import IDX_TO_LABEL\n",
    "input = next(iter(train_loader))\n",
    "print(\"Input ids(Decoded):\")\n",
    "for input_id in input[\"input_ids\"]:\n",
    "    print(tokenizer.decode(input_id))\n",
    "print(\"-\"*50)\n",
    "print(\"Attention mask:\")\n",
    "for attention_mask in input[\"attention_mask\"]:\n",
    "    print(attention_mask)\n",
    "print(\"-\"*50)\n",
    "print(\"Labels:\")\n",
    "for labels in input[\"labels\"]:\n",
    "    for label in labels:\n",
    "        print(IDX_TO_LABEL[label.item()], end=' ')\n",
    "    print()\n",
    "print(\"-\"*50)\n",
    "print(\"Word ids:\")\n",
    "for word_ids in input[\"word_ids\"]:\n",
    "    print(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding time: 0.04s\n",
      "Convert time: 0.03s\n",
      "CRF forward time: 0.03s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.8476, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.calculate_loss(**input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.predict(input['input_ids'], input['attention_mask'], input['word_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that no from S-xxx to M-*/E-*\n",
    "# no from B-xxx to B-*/M-yyy/E-yyy/O/<STOP>\n",
    "# no from M-xxx to B-*/M-yyy/E-yyy/O/<STOP>\n",
    "# no from E-* to M-*/E-*/\n",
    "# no from O to M-*/E-*\n",
    "# no from START_LABEL to M-*/E-*\n",
    "# no from B-*/M-* to STOP_LABEL\n",
    "for encoding in train_dataset:\n",
    "    labels = encoding['labels']\n",
    "    for t in range(len(labels) - 1):\n",
    "        from_entity = labels[t]\n",
    "        to_entity = labels[t + 1]\n",
    "        assert not (from_entity.startswith('S-') and (to_entity.startswith('M-') or to_entity.startswith('E-'))), tokenizer.decode(encoding['input_ids'])\n",
    "        assert not (from_entity.startswith('B-') and to_entity.startswith('B-')), tokenizer.decode(encoding['input_ids'])\n",
    "        if from_entity.startswith('B-') and (to_entity.startswith('M-') or to_entity.startswith('E-')):\n",
    "            assert from_entity[2:] == to_entity[2:], tokenizer.decode(encoding['input_ids'])\n",
    "        assert not (from_entity.startswith('M-') and to_entity.startswith('B-')), tokenizer.decode(encoding['input_ids'])\n",
    "        if from_entity.startswith('M-') and (to_entity.startswith('M-') or to_entity.startswith('E-')):\n",
    "            assert from_entity[2:] == to_entity[2:], tokenizer.decode(encoding['input_ids'])\n",
    "        assert not (from_entity.startswith('E-') and (to_entity.startswith('M-') or to_entity.startswith('E-'))), tokenizer.decode(encoding['input_ids'])\n",
    "        assert not (from_entity == 'O' and (to_entity.startswith('M-') or to_entity.startswith('E-'))), tokenizer.decode(encoding['input_ids'])\n",
    "        if t == 0: \n",
    "            assert not (from_entity.startswith('M-') or from_entity.startswith('E-'))\n",
    "        if t == len(labels) - 2:\n",
    "            assert not (to_entity.startswith('E-') or to_entity.startswith('M-'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1021, -0.0886, -0.0439,  0.0088,  0.0806,  0.0882, -0.0667,  0.0619,\n",
      "        -0.0218, -0.0558, -0.0643,  0.0889, -0.0395, -0.0266,  0.1135, -0.0393,\n",
      "        -0.0112, -0.0581,  0.0005, -0.0513], grad_fn=<SelectBackward0>)\n",
      "tensor([-0.1033, -0.0897, -0.0431,  0.0091,  0.0799,  0.0862, -0.0673,  0.0612,\n",
      "        -0.0228, -0.0544, -0.0627,  0.1047, -0.0451, -0.0353,  0.1374, -0.0383,\n",
      "        -0.0083, -0.0743,  0.0036, -0.0573], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "lstm_nobias = nn.LSTM(2, 10, 2, bidirectional=True, batch_first=True, bias=True)\n",
    "pad_1_step = torch.tensor([[1, 1],\n",
    "                    [0, 0]], dtype=torch.float32)\n",
    "pad_2_steps = torch.tensor([[1, 1],\n",
    "                    [0, 0],\n",
    "                    [1, 1]], dtype=torch.float32)\n",
    "\n",
    "# the bilstm output of the 1st time step\n",
    "print(lstm_nobias(pad_1_step)[0][0])\n",
    "print(lstm_nobias(pad_2_steps)[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "L, B = 4, 2\n",
    "a = torch.arange(8).view(4, 2)\n",
    "mask = torch.tensor([[2, 1],\n",
    "                    [1, 0],\n",
    "                    [1, 1],\n",
    "                    [0, 0]])\n",
    "a[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env http_proxy 127.0.0.1:7890\n",
    "%env https_proxy 127.0.0.1:7890\n",
    "import os\n",
    "from src.dataset import NERDataset\n",
    "from src.collator import NERDataCollator\n",
    "from src.pl_module import LightningBiLSTMCRF\n",
    "from src.variable import LABEL_TO_IDX\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, builder\n",
    "from pytorch_lightning import Trainer\n",
    "builder.has_sufficient_disk_space = lambda needed_bytes, directory=\".\": True\n",
    "print(\"Initializing tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"bert-base-chinese\")\n",
    "print(\"Initializing dataset...\")\n",
    "test_dataset = NERDataset('test', LABEL_TO_IDX, upsample=False)\n",
    "def test_generator():\n",
    "    for i in range(len(test_dataset)):\n",
    "        yield {\"text\": test_dataset.text[i]}\n",
    "test_dataset = Dataset.from_generator(test_generator)\n",
    "print(\"Tokenizing dataset...\")\n",
    "def tokenize(example):\n",
    "    encoding = tokenizer(example[\"text\"], is_split_into_words=True)\n",
    "    encoding['word_ids'] = [encoding.word_ids(b) for b in range(len(example['labels']))]\n",
    "    encoding['word_ids'] = [list(map(lambda x: -1 if x is None else x, word_id)) for word_id in encoding['word_ids']]\n",
    "    return encoding\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, remove_columns=[\"text\"], batch_size=32)\n",
    "print(\"Initializing dataloader...\")\n",
    "collator = NERDataCollator(tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, collate_fn=collator, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(test_dataloader))['input_ids'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import NERDataset\n",
    "from src.variable import LABEL_TO_IDX\n",
    "ds = NERDataset('train', LABEL_TO_IDX, upsample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: http_proxy=127.0.0.1:7890\n",
      "env: https_proxy=127.0.0.1:7890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ckiplab/bert-base-chinese-ner and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from src.pl_module import LightningBiLSTMCRF\n",
    "from transformers import BertTokenizerFast\n",
    "%env http_proxy 127.0.0.1:7890\n",
    "%env https_proxy 127.0.0.1:7890\n",
    "ckpt = LightningBiLSTMCRF.load_from_checkpoint('best_models/order/epoch=2-val_loss=0.0340-val_f1=0.8879.ckpt').to('cpu')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('ckiplab/bert-base-chinese-ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding time: 0.03s\n",
      "Convert time: 0.00s\n",
      "CRF decode time: 0.00s\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from src.variable import IDX_TO_LABEL\n",
    "text = \"警方逮捕了这个'电车之狼'\"\n",
    "encoding = tokenizer([text], return_tensors='pt')\n",
    "word_ids = encoding.word_ids(0)\n",
    "word_ids = list(map(lambda x: -1 if x is None else x, word_ids))\n",
    "word_ids = torch.tensor(word_ids).unsqueeze(0)\n",
    "pred = ckpt.model.predict(encoding['input_ids'], encoding['attention_mask'], word_ids)[0]\n",
    "pred = list(map(lambda x: IDX_TO_LABEL[x], pred))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From O to O: 0.1009884849190712\n",
      "From O to S-GPE: 0.09808619320392609\n",
      "From O to S-PER: 0.13781671226024628\n",
      "From O to B-ORG: 0.37097394466400146\n",
      "From O to E-ORG: -2.478883743286133\n",
      "From O to S-ORG: 0.23651066422462463\n",
      "From O to M-ORG: -2.2648699283599854\n",
      "From O to S-LOC: 0.2411877065896988\n",
      "From O to E-GPE: -1.6969536542892456\n",
      "From O to B-GPE: 0.08536899089813232\n",
      "From O to B-LOC: 0.1812945008277893\n",
      "From O to E-LOC: -1.3395737409591675\n",
      "From O to M-LOC: -1.0918552875518799\n",
      "From O to M-GPE: -1.4719953536987305\n",
      "From O to B-PER: 0.16019275784492493\n",
      "From O to E-PER: -1.585919976234436\n",
      "From O to M-PER: -0.7482620477676392\n",
      "From S-GPE to O: 0.12119190394878387\n",
      "From S-GPE to S-GPE: 0.28014716506004333\n",
      "From S-GPE to S-PER: -0.20965144038200378\n",
      "From S-GPE to B-ORG: 0.6756178140640259\n",
      "From S-GPE to E-ORG: -0.6303654313087463\n",
      "From S-GPE to S-ORG: 0.26704293489456177\n",
      "From S-GPE to M-ORG: -0.5888674855232239\n",
      "From S-GPE to S-LOC: 0.3229258954524994\n",
      "From S-GPE to E-GPE: -1.3272525072097778\n",
      "From S-GPE to B-GPE: -0.23715749382972717\n",
      "From S-GPE to B-LOC: 0.03953545168042183\n",
      "From S-GPE to E-LOC: -0.3472558557987213\n",
      "From S-GPE to M-LOC: -0.17309048771858215\n",
      "From S-GPE to M-GPE: -0.5114546418190002\n",
      "From S-GPE to B-PER: 0.017856426537036896\n",
      "From S-GPE to E-PER: -0.13432571291923523\n",
      "From S-GPE to M-PER: -0.09100644290447235\n",
      "From S-PER to O: 0.07599788159132004\n",
      "From S-PER to S-GPE: 0.21285225450992584\n",
      "From S-PER to S-PER: -0.14106962084770203\n",
      "From S-PER to B-ORG: 0.03827647864818573\n",
      "From S-PER to E-ORG: -0.4336284399032593\n",
      "From S-PER to S-ORG: -0.042475536465644836\n",
      "From S-PER to M-ORG: -0.27918145060539246\n",
      "From S-PER to S-LOC: -0.11909462511539459\n",
      "From S-PER to E-GPE: -0.03870827704668045\n",
      "From S-PER to B-GPE: -0.0369267575442791\n",
      "From S-PER to B-LOC: 0.023157693445682526\n",
      "From S-PER to E-LOC: -0.02554725855588913\n",
      "From S-PER to M-LOC: -0.12122374773025513\n",
      "From S-PER to M-GPE: -0.060486264526844025\n",
      "From S-PER to B-PER: -0.06483010202646255\n",
      "From S-PER to E-PER: -0.6230990886688232\n",
      "From S-PER to M-PER: -0.4240460991859436\n",
      "From B-ORG to O: -2.648303747177124\n",
      "From B-ORG to S-GPE: -0.4847472906112671\n",
      "From B-ORG to S-PER: -0.6128680109977722\n",
      "From B-ORG to B-ORG: -1.4848358631134033\n",
      "From B-ORG to E-ORG: 1.1136398315429688\n",
      "From B-ORG to S-ORG: -1.4321341514587402\n",
      "From B-ORG to M-ORG: 0.9370725154876709\n",
      "From B-ORG to S-LOC: -0.18247108161449432\n",
      "From B-ORG to E-GPE: -0.09447162598371506\n",
      "From B-ORG to B-GPE: -0.1916295886039734\n",
      "From B-ORG to B-LOC: -0.15956436097621918\n",
      "From B-ORG to E-LOC: -0.2029033899307251\n",
      "From B-ORG to M-LOC: -0.12656474113464355\n",
      "From B-ORG to M-GPE: -0.07499731332063675\n",
      "From B-ORG to B-PER: -0.09828344732522964\n",
      "From B-ORG to E-PER: -0.12511152029037476\n",
      "From B-ORG to M-PER: -0.06156320869922638\n",
      "From E-ORG to O: 0.3737509250640869\n",
      "From E-ORG to S-GPE: 0.034117285162210464\n",
      "From E-ORG to S-PER: 0.3173891305923462\n",
      "From E-ORG to B-ORG: -0.20407064259052277\n",
      "From E-ORG to E-ORG: -0.7892774343490601\n",
      "From E-ORG to S-ORG: -0.20451484620571136\n",
      "From E-ORG to M-ORG: -0.8663162589073181\n",
      "From E-ORG to S-LOC: -0.09981994330883026\n",
      "From E-ORG to E-GPE: -0.1685567945241928\n",
      "From E-ORG to B-GPE: -0.009345104917883873\n",
      "From E-ORG to B-LOC: -0.10014552623033524\n",
      "From E-ORG to E-LOC: -0.11964388936758041\n",
      "From E-ORG to M-LOC: -0.06825396418571472\n",
      "From E-ORG to M-GPE: -0.03287794813513756\n",
      "From E-ORG to B-PER: -0.029818184673786163\n",
      "From E-ORG to E-PER: -0.1507936269044876\n",
      "From E-ORG to M-PER: 0.028822558000683784\n",
      "From S-ORG to O: 0.32674553990364075\n",
      "From S-ORG to S-GPE: 0.13853445649147034\n",
      "From S-ORG to S-PER: 0.3851930797100067\n",
      "From S-ORG to B-ORG: 0.1754922717809677\n",
      "From S-ORG to E-ORG: -1.4771173000335693\n",
      "From S-ORG to S-ORG: 0.07201571017503738\n",
      "From S-ORG to M-ORG: -1.2754548788070679\n",
      "From S-ORG to S-LOC: -0.062178466469049454\n",
      "From S-ORG to E-GPE: -0.13406045734882355\n",
      "From S-ORG to B-GPE: -0.10532833635807037\n",
      "From S-ORG to B-LOC: -0.10621603578329086\n",
      "From S-ORG to E-LOC: -0.12795743346214294\n",
      "From S-ORG to M-LOC: -0.10722356289625168\n",
      "From S-ORG to M-GPE: -0.14651460945606232\n",
      "From S-ORG to B-PER: 0.010353559628129005\n",
      "From S-ORG to E-PER: -0.2634752690792084\n",
      "From S-ORG to M-PER: -0.059955451637506485\n",
      "From M-ORG to O: -2.109140634536743\n",
      "From M-ORG to S-GPE: -0.29286453127861023\n",
      "From M-ORG to S-PER: -0.45927828550338745\n",
      "From M-ORG to B-ORG: -0.9538429379463196\n",
      "From M-ORG to E-ORG: 0.7655262351036072\n",
      "From M-ORG to S-ORG: -0.941516637802124\n",
      "From M-ORG to M-ORG: 0.650506854057312\n",
      "From M-ORG to S-LOC: -0.03608136251568794\n",
      "From M-ORG to E-GPE: -0.05691775679588318\n",
      "From M-ORG to B-GPE: -0.13086847960948944\n",
      "From M-ORG to B-LOC: -0.059836022555828094\n",
      "From M-ORG to E-LOC: -0.1814374178647995\n",
      "From M-ORG to M-LOC: -0.021334361284971237\n",
      "From M-ORG to M-GPE: -0.03664112836122513\n",
      "From M-ORG to B-PER: -0.015217693522572517\n",
      "From M-ORG to E-PER: -0.04389987513422966\n",
      "From M-ORG to M-PER: -0.04130936414003372\n",
      "From S-LOC to O: 0.4088825285434723\n",
      "From S-LOC to S-GPE: -0.0019385818159207702\n",
      "From S-LOC to S-PER: -0.07011068612337112\n",
      "From S-LOC to B-ORG: 0.09048033505678177\n",
      "From S-LOC to E-ORG: -0.21265098452568054\n",
      "From S-LOC to S-ORG: -0.07505210489034653\n",
      "From S-LOC to M-ORG: -0.4135546386241913\n",
      "From S-LOC to S-LOC: 0.20794041454792023\n",
      "From S-LOC to E-GPE: -0.012659875676035881\n",
      "From S-LOC to B-GPE: -0.0664750337600708\n",
      "From S-LOC to B-LOC: -0.12434416264295578\n",
      "From S-LOC to E-LOC: -0.7706131935119629\n",
      "From S-LOC to M-LOC: -0.45022863149642944\n",
      "From S-LOC to M-GPE: -0.13133887946605682\n",
      "From S-LOC to B-PER: -0.02503851428627968\n",
      "From S-LOC to E-PER: -0.020497482270002365\n",
      "From S-LOC to M-PER: -0.04609079658985138\n",
      "From E-GPE to O: 0.05512883886694908\n",
      "From E-GPE to S-GPE: -0.16548176109790802\n",
      "From E-GPE to S-PER: 0.18588365614414215\n",
      "From E-GPE to B-ORG: -0.06892652064561844\n",
      "From E-GPE to E-ORG: -0.11807592958211899\n",
      "From E-GPE to S-ORG: -0.08909155428409576\n",
      "From E-GPE to M-ORG: -0.010787085629999638\n",
      "From E-GPE to S-LOC: 0.12284735590219498\n",
      "From E-GPE to E-GPE: -0.6403505206108093\n",
      "From E-GPE to B-GPE: -0.1362985372543335\n",
      "From E-GPE to B-LOC: -0.01268890779465437\n",
      "From E-GPE to E-LOC: -0.096873939037323\n",
      "From E-GPE to M-LOC: 0.030322300270199776\n",
      "From E-GPE to M-GPE: -0.23211954534053802\n",
      "From E-GPE to B-PER: -0.044963907450437546\n",
      "From E-GPE to E-PER: 0.008761460892856121\n",
      "From E-GPE to M-PER: 0.0037984007503837347\n",
      "From B-GPE to O: -1.9160215854644775\n",
      "From B-GPE to S-GPE: -1.1221693754196167\n",
      "From B-GPE to S-PER: -0.14939308166503906\n",
      "From B-GPE to B-ORG: -0.17406828701496124\n",
      "From B-GPE to E-ORG: -0.11817898601293564\n",
      "From B-GPE to S-ORG: -0.3139427602291107\n",
      "From B-GPE to M-ORG: -0.2307698130607605\n",
      "From B-GPE to S-LOC: -0.15537454187870026\n",
      "From B-GPE to E-GPE: 0.8403130173683167\n",
      "From B-GPE to B-GPE: -0.6494401097297668\n",
      "From B-GPE to B-LOC: -0.17123879492282867\n",
      "From B-GPE to E-LOC: -0.1663188636302948\n",
      "From B-GPE to M-LOC: -0.15001581609249115\n",
      "From B-GPE to M-GPE: 0.6632832884788513\n",
      "From B-GPE to B-PER: -0.06415509432554245\n",
      "From B-GPE to E-PER: -0.016035960987210274\n",
      "From B-GPE to M-PER: -0.11978254467248917\n",
      "From B-LOC to O: -1.4722356796264648\n",
      "From B-LOC to S-GPE: -0.24318693578243256\n",
      "From B-LOC to S-PER: -0.035850781947374344\n",
      "From B-LOC to B-ORG: -0.05293748155236244\n",
      "From B-LOC to E-ORG: -0.23100045323371887\n",
      "From B-LOC to S-ORG: -0.17572595179080963\n",
      "From B-LOC to M-ORG: -0.27984490990638733\n",
      "From B-LOC to S-LOC: -0.8327842950820923\n",
      "From B-LOC to E-GPE: -0.21069946885108948\n",
      "From B-LOC to B-GPE: 0.028757300227880478\n",
      "From B-LOC to B-LOC: -0.8256286382675171\n",
      "From B-LOC to E-LOC: 0.6367979645729065\n",
      "From B-LOC to M-LOC: 0.5128400325775146\n",
      "From B-LOC to M-GPE: -0.1163802519440651\n",
      "From B-LOC to B-PER: 0.02136709727346897\n",
      "From B-LOC to E-PER: -0.07765199989080429\n",
      "From B-LOC to M-PER: 0.00011128825281048194\n",
      "From E-LOC to O: 0.23160025477409363\n",
      "From E-LOC to S-GPE: 0.04115784913301468\n",
      "From E-LOC to S-PER: -0.0508044958114624\n",
      "From E-LOC to B-ORG: -0.009949044324457645\n",
      "From E-LOC to E-ORG: -0.10725575685501099\n",
      "From E-LOC to S-ORG: -0.034383196383714676\n",
      "From E-LOC to M-ORG: -0.053427599370479584\n",
      "From E-LOC to S-LOC: -0.3000078499317169\n",
      "From E-LOC to E-GPE: -0.04979163780808449\n",
      "From E-LOC to B-GPE: 0.01485888659954071\n",
      "From E-LOC to B-LOC: -0.001937962369993329\n",
      "From E-LOC to E-LOC: -0.5954257845878601\n",
      "From E-LOC to M-LOC: -0.3009640574455261\n",
      "From E-LOC to M-GPE: -0.0056188940070569515\n",
      "From E-LOC to B-PER: -0.05479350686073303\n",
      "From E-LOC to E-PER: -0.10221465677022934\n",
      "From E-LOC to M-PER: 0.0025703131686896086\n",
      "From M-LOC to O: -1.0328795909881592\n",
      "From M-LOC to S-GPE: -0.16023682057857513\n",
      "From M-LOC to S-PER: 0.024431319907307625\n",
      "From M-LOC to B-ORG: -0.09221201390028\n",
      "From M-LOC to E-ORG: -0.1193157434463501\n",
      "From M-LOC to S-ORG: -0.1011623665690422\n",
      "From M-LOC to M-ORG: -0.16166658699512482\n",
      "From M-LOC to S-LOC: -0.41325506567955017\n",
      "From M-LOC to E-GPE: -0.1641923189163208\n",
      "From M-LOC to B-GPE: 0.033838849514722824\n",
      "From M-LOC to B-LOC: -0.38075894117355347\n",
      "From M-LOC to E-LOC: 0.6017277240753174\n",
      "From M-LOC to M-LOC: 0.21896637976169586\n",
      "From M-LOC to M-GPE: 0.017048971727490425\n",
      "From M-LOC to B-PER: 0.014392460696399212\n",
      "From M-LOC to E-PER: -0.12076966464519501\n",
      "From M-LOC to M-PER: 0.0100430678576231\n",
      "From M-GPE to O: -1.436928629875183\n",
      "From M-GPE to S-GPE: -0.5874960422515869\n",
      "From M-GPE to S-PER: -0.33699631690979004\n",
      "From M-GPE to B-ORG: -0.09952352941036224\n",
      "From M-GPE to E-ORG: -0.12560302019119263\n",
      "From M-GPE to S-ORG: -0.125339537858963\n",
      "From M-GPE to M-ORG: -0.1598123461008072\n",
      "From M-GPE to S-LOC: -0.13313046097755432\n",
      "From M-GPE to E-GPE: 0.5826469659805298\n",
      "From M-GPE to B-GPE: -0.11328873783349991\n",
      "From M-GPE to B-LOC: -0.04501989856362343\n",
      "From M-GPE to E-LOC: -0.2035849541425705\n",
      "From M-GPE to M-LOC: -0.06537187844514847\n",
      "From M-GPE to M-GPE: 0.2318885773420334\n",
      "From M-GPE to B-PER: -0.10898413509130478\n",
      "From M-GPE to E-PER: -0.12653328478336334\n",
      "From M-GPE to M-PER: -0.10623317211866379\n",
      "From B-PER to O: -1.5580964088439941\n",
      "From B-PER to S-GPE: -0.07360217720270157\n",
      "From B-PER to S-PER: -0.8730352520942688\n",
      "From B-PER to B-ORG: -0.04227757081389427\n",
      "From B-PER to E-ORG: -0.14015458524227142\n",
      "From B-PER to S-ORG: -0.12143886834383011\n",
      "From B-PER to M-ORG: -0.13948357105255127\n",
      "From B-PER to S-LOC: -0.060192618519067764\n",
      "From B-PER to E-GPE: -0.0029284462798386812\n",
      "From B-PER to B-GPE: -0.052838411182165146\n",
      "From B-PER to B-LOC: 0.0015759289963170886\n",
      "From B-PER to E-LOC: -0.09448383003473282\n",
      "From B-PER to M-LOC: -0.0032920169178396463\n",
      "From B-PER to M-GPE: -0.0004231581697240472\n",
      "From B-PER to B-PER: -0.6168829798698425\n",
      "From B-PER to E-PER: 0.7476232051849365\n",
      "From B-PER to M-PER: 0.6112270951271057\n",
      "From E-PER to O: 0.14504674077033997\n",
      "From E-PER to S-GPE: -0.059213150292634964\n",
      "From E-PER to S-PER: -0.10547221451997757\n",
      "From E-PER to B-ORG: -0.0665675550699234\n",
      "From E-PER to E-ORG: -0.05185067653656006\n",
      "From E-PER to S-ORG: 0.03224648907780647\n",
      "From E-PER to M-ORG: -0.0984196588397026\n",
      "From E-PER to S-LOC: 0.05167246609926224\n",
      "From E-PER to E-GPE: -0.07067817449569702\n",
      "From E-PER to B-GPE: -0.10452733188867569\n",
      "From E-PER to B-LOC: -0.0037417022977024317\n",
      "From E-PER to E-LOC: 0.03915172815322876\n",
      "From E-PER to M-LOC: -0.0955345407128334\n",
      "From E-PER to M-GPE: -0.07169247418642044\n",
      "From E-PER to B-PER: -0.14255566895008087\n",
      "From E-PER to E-PER: -0.5084564089775085\n",
      "From E-PER to M-PER: -0.31596359610557556\n",
      "From M-PER to O: -1.090806007385254\n",
      "From M-PER to S-GPE: -0.10048229247331619\n",
      "From M-PER to S-PER: -0.5389639735221863\n",
      "From M-PER to B-ORG: 0.019196687266230583\n",
      "From M-PER to E-ORG: 0.000980712124146521\n",
      "From M-PER to S-ORG: -0.0255091842263937\n",
      "From M-PER to M-ORG: -0.13143832981586456\n",
      "From M-PER to S-LOC: -0.11597749590873718\n",
      "From M-PER to E-GPE: -0.05348587408661842\n",
      "From M-PER to B-GPE: -0.042747922241687775\n",
      "From M-PER to B-LOC: 0.003287925850600004\n",
      "From M-PER to E-LOC: -0.13373373448848724\n",
      "From M-PER to M-LOC: -0.08513790369033813\n",
      "From M-PER to M-GPE: -0.03708134964108467\n",
      "From M-PER to B-PER: -0.3598897159099579\n",
      "From M-PER to E-PER: 0.5710579752922058\n",
      "From M-PER to M-PER: 0.2626487910747528\n"
     ]
    }
   ],
   "source": [
    "for start_label in range(len(IDX_TO_LABEL)):\n",
    "    for end_label in range(len(IDX_TO_LABEL)):\n",
    "        print(f\"From {IDX_TO_LABEL[start_label]} to {IDX_TO_LABEL[end_label]}: {ckpt.model.crf.transitions[start_label, end_label].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
