{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, builder\n",
    "from pytorch_lightning import Trainer\n",
    "from argparse import ArgumentParser\n",
    "from src.dataset import NERDataset\n",
    "from src.collator import NERDataCollator\n",
    "from src.pl_module import LightningBiLSTMCRF\n",
    "from src.variable import LABEL_TO_IDX, PAD_LABEL\n",
    "builder.has_sufficient_disk_space = lambda needed_bytes, directory=\".\": True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env http_proxy 127.0.0.1:7890\n",
    "%env https_proxy 127.0.0.1:7890\n",
    "print(\"Initializing tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('ckiplab/bert-base-chinese-ner')\n",
    "print(\"Initializing dataset...\")\n",
    "train_dataset_name = \"toy\"\n",
    "train_dataset = NERDataset(train_dataset_name, LABEL_TO_IDX, upsample=False)\n",
    "val_dataset_name = \"dev\"\n",
    "val_dataset = NERDataset(val_dataset_name, LABEL_TO_IDX, upsample=False)\n",
    "def train_generator():\n",
    "    for i in range(len(train_dataset)):\n",
    "        yield {\"text\": train_dataset.text[i], \"labels\": train_dataset.labels[i]}\n",
    "def val_generator():\n",
    "    for i in range(len(val_dataset)):\n",
    "        yield {\"text\": val_dataset.text[i], \"labels\": val_dataset.labels[i]}\n",
    "train_dataset = Dataset.from_generator(train_generator)\n",
    "val_dataset = Dataset.from_generator(val_generator)\n",
    "print(\"Tokenizing dataset...\")\n",
    "def tokenize(example):\n",
    "    encoding = tokenizer(example[\"text\"], is_split_into_words=True)\n",
    "    encoding['word_ids'] = [encoding.word_ids(b) for b in range(len(example['labels']))]\n",
    "    encoding['word_ids'] = [list(map(lambda x: -1 if x is None else x, word_id)) for word_id in encoding['word_ids']]\n",
    "    encoding['labels'] = [[LABEL_TO_IDX[y] for y in b] for b in example['labels']]\n",
    "    return encoding\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=32, remove_columns=[\"text\"])\n",
    "val_dataset = val_dataset.map(tokenize, batched=True, batch_size=32, remove_columns=[\"text\"])\n",
    "collator = NERDataCollator(tokenizer=tokenizer)\n",
    "train_loader = DataLoader(train_dataset, collate_fn=collator, batch_size=32, num_workers=47, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, collate_fn=collator, batch_size=32, num_workers=47)\n",
    "print(\"Initializing model...\")\n",
    "model = LightningBiLSTMCRF(LABEL_TO_IDX, 1, 256, \n",
    "                        bert_lr=0.0, lr=3e-5,\n",
    "                        optimizer='adamw', scheduler='onecycle',\n",
    "                        pretrained_model_name='ckiplab/bert-base-chinese-ner', freeze_bert=True,\n",
    "                        epochs=20, steps_per_epoch=len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids(Decoded):\n",
      "[CLS] 上 海 市 市 长 。 [SEP]\n",
      "[CLS] 腾 讯 游 戏 [SEP]\n",
      "Attention mask:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1]\n",
      "Word ids:\n",
      "[-1, 0, 0, 0, 1, 1, 2, -1]\n",
      "[-1, 0, 1, 2, 2, -1]\n",
      "labels:\n",
      "[1, 0, 0]\n",
      "[3, 6, 4]\n"
     ]
    }
   ],
   "source": [
    "print(\"Input ids(Decoded):\")\n",
    "for data in train_dataset:\n",
    "    print(tokenizer.decode(data['input_ids']))\n",
    "print(\"Attention mask:\")\n",
    "for data in train_dataset:\n",
    "    print(data['attention_mask'])\n",
    "print(\"Word ids:\")\n",
    "for data in train_dataset:\n",
    "    print(data['word_ids'])\n",
    "print(\"labels:\")\n",
    "for data in train_dataset:\n",
    "    print(data['labels'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids(Decoded):\n",
      "[CLS] 上 海 市 市 长 。 [SEP]\n",
      "[CLS] 腾 讯 游 戏 [SEP] [PAD] [PAD]\n",
      "--------------------------------------------------\n",
      "Attention mask:\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 0, 0])\n",
      "--------------------------------------------------\n",
      "Labels:\n",
      "S-GPE O O \n",
      "B-ORG M-ORG E-ORG \n",
      "--------------------------------------------------\n",
      "Word ids:\n",
      "tensor([-1,  0,  0,  0,  1,  1,  2, -1])\n",
      "tensor([-1,  0,  1,  2,  2, -1, -1, -1])\n"
     ]
    }
   ],
   "source": [
    "from src.variable import IDX_TO_LABEL\n",
    "input = next(iter(train_loader))\n",
    "print(\"Input ids(Decoded):\")\n",
    "for input_id in input[\"input_ids\"]:\n",
    "    print(tokenizer.decode(input_id))\n",
    "print(\"-\"*50)\n",
    "print(\"Attention mask:\")\n",
    "for attention_mask in input[\"attention_mask\"]:\n",
    "    print(attention_mask)\n",
    "print(\"-\"*50)\n",
    "print(\"Labels:\")\n",
    "for labels in input[\"labels\"]:\n",
    "    for label in labels:\n",
    "        print(IDX_TO_LABEL[label.item()], end=' ')\n",
    "    print()\n",
    "print(\"-\"*50)\n",
    "print(\"Word ids:\")\n",
    "for word_ids in input[\"word_ids\"]:\n",
    "    print(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding time: 0.04s\n",
      "Convert time: 0.03s\n",
      "CRF forward time: 0.03s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.8476, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.calculate_loss(**input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.predict(input['input_ids'], input['attention_mask'], input['word_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that no from S-xxx to M-*/E-*\n",
    "# no from B-xxx to B-*/M-yyy/E-yyy/O/<STOP>\n",
    "# no from M-xxx to B-*/M-yyy/E-yyy/O/<STOP>\n",
    "# no from E-* to M-*/E-*/\n",
    "# no from O to M-*/E-*\n",
    "# no from START_LABEL to M-*/E-*\n",
    "# no from B-*/M-* to STOP_LABEL\n",
    "for encoding in train_dataset:\n",
    "    labels = encoding['labels']\n",
    "    for t in range(len(labels) - 1):\n",
    "        from_entity = labels[t]\n",
    "        to_entity = labels[t + 1]\n",
    "        assert not (from_entity.startswith('S-') and (to_entity.startswith('M-') or to_entity.startswith('E-'))), tokenizer.decode(encoding['input_ids'])\n",
    "        assert not (from_entity.startswith('B-') and to_entity.startswith('B-')), tokenizer.decode(encoding['input_ids'])\n",
    "        if from_entity.startswith('B-') and (to_entity.startswith('M-') or to_entity.startswith('E-')):\n",
    "            assert from_entity[2:] == to_entity[2:], tokenizer.decode(encoding['input_ids'])\n",
    "        assert not (from_entity.startswith('M-') and to_entity.startswith('B-')), tokenizer.decode(encoding['input_ids'])\n",
    "        if from_entity.startswith('M-') and (to_entity.startswith('M-') or to_entity.startswith('E-')):\n",
    "            assert from_entity[2:] == to_entity[2:], tokenizer.decode(encoding['input_ids'])\n",
    "        assert not (from_entity.startswith('E-') and (to_entity.startswith('M-') or to_entity.startswith('E-'))), tokenizer.decode(encoding['input_ids'])\n",
    "        assert not (from_entity == 'O' and (to_entity.startswith('M-') or to_entity.startswith('E-'))), tokenizer.decode(encoding['input_ids'])\n",
    "        if t == 0: \n",
    "            assert not (from_entity.startswith('M-') or from_entity.startswith('E-'))\n",
    "        if t == len(labels) - 2:\n",
    "            assert not (to_entity.startswith('E-') or to_entity.startswith('M-'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "lstm = nn.LSTM(2, 10, 2, bidirectional=True, batch_first=True, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor([[1, 1],\n",
    "                    [0, 0]], dtype=torch.float32)\n",
    "lstm(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "L, B = 4, 2\n",
    "a = torch.arange(8).view(4, 2)\n",
    "mask = torch.tensor([[2, 1],\n",
    "                    [1, 0],\n",
    "                    [1, 1],\n",
    "                    [0, 0]])\n",
    "a[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env http_proxy 127.0.0.1:7890\n",
    "%env https_proxy 127.0.0.1:7890\n",
    "import os\n",
    "from src.dataset import NERDataset\n",
    "from src.collator import NERDataCollator\n",
    "from src.pl_module import LightningBiLSTMCRF\n",
    "from src.variable import LABEL_TO_IDX\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, builder\n",
    "from pytorch_lightning import Trainer\n",
    "builder.has_sufficient_disk_space = lambda needed_bytes, directory=\".\": True\n",
    "print(\"Initializing tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"bert-base-chinese\")\n",
    "print(\"Initializing dataset...\")\n",
    "test_dataset = NERDataset('test', LABEL_TO_IDX, upsample=False)\n",
    "def test_generator():\n",
    "    for i in range(len(test_dataset)):\n",
    "        yield {\"text\": test_dataset.text[i]}\n",
    "test_dataset = Dataset.from_generator(test_generator)\n",
    "print(\"Tokenizing dataset...\")\n",
    "def tokenize(example):\n",
    "    encoding = tokenizer(example[\"text\"], is_split_into_words=True)\n",
    "    encoding['word_ids'] = [encoding.word_ids(b) for b in range(len(example['labels']))]\n",
    "    encoding['word_ids'] = [list(map(lambda x: -1 if x is None else x, word_id)) for word_id in encoding['word_ids']]\n",
    "    return encoding\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, remove_columns=[\"text\"], batch_size=32)\n",
    "print(\"Initializing dataloader...\")\n",
    "collator = NERDataCollator(tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, collate_fn=collator, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(test_dataloader))['input_ids'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import NERDataset\n",
    "from src.variable import LABEL_TO_IDX\n",
    "ds = NERDataset('train', LABEL_TO_IDX, upsample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env http_proxy 127.0.0.1:7890\n",
    "%env https_proxy 127.0.0.1:7890\n",
    "from transformers import (\n",
    "  BertTokenizerFast,\n",
    "  AutoModel,\n",
    ")\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "model = AutoModel.from_pretrained('ckiplab/bert-base-chinese-ner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(**tokenizer('我叫沃尔夫冈，我住在柏林。', return_tensors='pt')).last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_id in tokenizer('shanghai').input_ids:\n",
    "    print(tokenizer.decode(input_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "input_ids = tokenizer([\"Héllò hôw\", \"are\", \"ü?\"], is_split_into_words=True).input_ids\n",
    "# input_ids = tokenizer([\"Héllò hôw are ü?\"]).input_ids\n",
    "for input_id in input_ids:\n",
    "    print(tokenizer.decode(input_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
